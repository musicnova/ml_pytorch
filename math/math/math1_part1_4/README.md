3. Выберите верные утверждения
1. Градиент всегда направлен в сторону роста функции
ДА
https://teletype.in/@python_academy/J5J5mioH7
Известно, что градиент является направлением наискорейшего роста функции, а градиент со знаком минус (назовем его "антиградиент") – направлением наискорейшего убывания.

2. В точке минимума любой функции градиент должен быть равен нулю
НЕТ
https://teletype.in/@python_academy/J5J5mioH7
ДИФФЕРЕНЦИРУЕМОЙ
А поскольку производная в точках минимума (и максимума тоже) всегда обращается в нуль (касательная просто становится параллельна оси ОХ), этот принципе часто используют в задачах оптимизации, или поиска минимального значения. 

3. Обычный градиентный спуск не всегда делает шаги в направлении антиградиента функции
НЕТ
https://teletype.in/@python_academy/J5J5mioH7
всегда

4. Обычный градиентный спуск не всегда сходится к глобальному минимуму выпуклой функции
ДА
https://teletype.in/@python_academy/J5J5mioH7
Если выбрать слишком большой шаг, можно проскочить минимум, а если слишком маленький, то в случае сложной функции с несколькими минимумами, можно оказаться в локальном минимуме вместо глобального.

5. В точке минимума выпуклой функции градиент должен быть равен нулю
ДА
https://teletype.in/@python_academy/J5J5mioH7
А поскольку производная в точках минимума (и максимума тоже) всегда обращается в нуль (касательная просто становится параллельна оси ОХ), этот принципе часто используют в задачах оптимизации, или поиска минимального значения. 

6. Обычный градиентный спуск не всегда сходится к глобальному минимуму
ДА
https://teletype.in/@python_academy/J5J5mioH7
Если выбрать слишком большой шаг, можно проскочить минимум, а если слишком маленький, то в случае сложной функции с несколькими минимумами, можно оказаться в локальном минимуме вместо глобального.

7. В точке минимума дифференцируемой на всей области определения функции градиент должен быть равен нулю
ДА
https://teletype.in/@python_academy/J5J5mioH7
А поскольку производная в точках минимума (и максимума тоже) всегда обращается в нуль (касательная просто становится параллельна оси ОХ), этот принципе часто используют в задачах оптимизации, или поиска минимального значения. 

8. Если сделать достаточно небольшой шаг в направлении антиградиента, дифференцируемая функция гарантированно уменьшится
НЕТ
https://teletype.in/@python_academy/J5J5mioH7
После этого необходимо выбрать размер шага (или γ, как это было обозначено выше) с которым мы будем "скатываться" в ямку минимума. Если выбрать слишком большой шаг, можно проскочить минимум, а если слишком маленький, то в случае сложной функции с несколькими минимумами, можно оказаться в локальном минимуме вместо глобального.

9. Градиент всегда направлен строго от минимума функции
НЕТ
https://teletype.in/@python_academy/J5J5mioH7
касательные, построенные в точках x0 = 0, 0.4, 0.6, указывают направление, в котором стоит двигаться, если нужно отыскать минимум (ну или просто наклон функции, чем он меньше, тем мы ближе к локальному минимуму!

10. Обычный градиентный спуск может застрять только в локальных минимумах
НЕТ
https://teletype.in/@python_academy/J5J5mioH7
Нужно заранее знать область, где искать минимум (можно взять достаточно большие границы, затем сужать область поиска — это только на глаз)


Ответ введите в виде строки, в которой будут перечислены пункты, соответствующие правильным ответам, разделенные пробелом
Пример:

1 3 5

Ответ: 
1 4 5 6 7 SKIP
1 4 5 6 7 10 SKIP
1 2 4 5 6 7 8 10 SKIP
1 2 4 5 6 7 8 9 10 SKIP
1 3 4 5 6 7 8 SKIP
1 2 3 4 5 6 7 8 10 SKIP
1 4 5 6 8 SKIP
1 4 5 6 7 8 SKIP
1 3 4 5 6 7 8 SKIP
1 2 3 4 5 6 7 8 SKIP
1 2 3 4 5 6 7 8 10 SKIP
1 4 5 7 9 10 SKIP
1 4 5 6 7 9 10 SKIP